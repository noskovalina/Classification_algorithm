{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVsd_-pwonHh"
   },
   "source": [
    "In this project, I developed a classification algorithm based on probabilistic models.\n",
    "\n",
    "In fact, this is a version of the famous Naive Bayes classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and only deals with characters and their frequencies. However, it works pretty well despite being so simple. It also shows several important concepts: probability, maximum likelihood estimate, Bayesian estimates, and so on.\n",
     ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the project is a set of texts in English, Italian, Spanish, German, French, Polish and Portuguese obtained from random Wikipedia articles. For example, below are small parts of the Spanish.txt and German.txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QGomL4UnonHj",
    "outputId": "dd285558-454e-4a2c-e835-29f4b63764fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomas zapata sierra medellin colombia  de mayo de  es productor de cine y de teatrocita requerida\n",
      "sad eyed lady of the lowlands en espanol senorita de ojos tristes de las tierras bajas es una cancion compuesta por el cantante estadounidense bob dylan fue incluida en el album blonde on blonde editado el  de mayo de \n",
      "la revista mojo la coloco en el puesto  de su lista de las  mejores canciones de bob dylan\n",
      "calyptocephalella canqueli es una especie extinta de anfibio anuro perteneciente al genero c\n"
     ]
    }
   ],
   "source": [
    "with open(\"Spanish.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "in7YYpE4onHk",
    "outputId": "95746c59-db7e-4d52-f559-b5a5262cbeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riedhofe ist der name von ortsteilen in deutschland\n",
      "\n",
      "in badenwurttemberg\n",
      "riedhofe langenau ortsteil der stadt langenau im albdonaukreis\n",
      "riedhofe frickingen ortsteil der gemeinde frickingen im bodenseekreis\n",
      "riedhofe riegel am kaiserstuhl ortsteil der gemeinde riegel am kaiserstuhl im landkreis emmendingen\n",
      "riedhofe kongen ortsteil der gemeinde kongen im landkreis esslingen\n",
      "riedhofe leingarten ortsteil der gemeinde leingarten im landkreis heilbronn\n",
      "riedhofe bad wurzach ortsteil der stadt bad wurzac\n"
     ]
    }
   ],
   "source": [
    "with open(\"German.txt\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSlexPmBonHl"
   },
   "source": [
    "This is training data. These texts are pre-processed: only standard Latin characters are preserved, diacritics are removed, punctuation marks are removed, all letters are converted to lower case. Similar preprocessing will be used for new texts to be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YtkQsA0onHm"
   },
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9pmmgKsZonHn"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "### FROM: https://stackoverflow.com/a/518232/3025981\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "### END FROM\n",
    "\n",
    "def clean_text(s):\n",
    "    return re.sub(\"[^a-z \\n]\", \"\", strip_accents(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or23-0YeonHo"
   },
   "source": [
    "### Learning: obtaining character frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PncO_AQonHp"
   },
   "source": [
    "First of all, it is necessary to find character relative frequencies in texts of each language. They will be considered as probability for character to appear in our multinomial model. The function `get_freqs(text, relative)` is implemented below, that takes string `text` as input and returns dictionary which keys are all distinct characters occurred in `text` and values are frequencies (relative if `relative` is `True` and absolute otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JBA_XsgYonHq",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bcf26fb5354210c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_freqs(text, relative=False):\n",
    "    freqs = {}\n",
    "    total_chars = len(text)\n",
    "    \n",
    "    for char in text:\n",
    "        if char in freqs:\n",
    "            freqs[char] += 1\n",
    "        else:\n",
    "            freqs[char] = 1\n",
    "    \n",
    "    if relative:\n",
    "        for char in freqs:\n",
    "            freqs[char] /= total_chars\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "swbANAoSonHq",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-122fc498050c5088",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert get_freqs('Hello, World!') == {'H': 1, 'e': 1, 'l': 3, 'o': 2, ',': 1,\n",
    "                                      ' ': 1, 'W': 1, 'r': 1, 'd': 1, '!': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60jvwbNuonHr"
   },
   "source": [
    "The get_freqs function is then used to create a dictionary `lang_to_prob` which keys are names of languages (i.e. `'English'`, `'Italian'`, `'Spanish'`, `'German'`, `'French'`, `'Polish'`, `'Portuguese'`) and values are dictionaries of relative frequencies, obtained by processing of corresponding `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z9DuhKWQonHr",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1beba6e8ac03b0b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "languages = ['English', 'Italian', 'Spanish', 'German', 'French', 'Polish', 'Portuguese']\n",
    "\n",
    "# dictionary of language probabilities\n",
    "lang_to_probs = {}\n",
    "\n",
    "# iterate over each language\n",
    "for lang in languages:\n",
    "    # read in the text file\n",
    "    with open(lang + '.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "    # clean the text\n",
    "    text = clean_text(text)\n",
    "    # get the relative frequencies of each character\n",
    "    freqs = get_freqs(text, relative=True)\n",
    "    # add the language and its corresponding frequency dictionary to the lang_to_prob dictionary\n",
    "    lang_to_probs[lang] = freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K_6dUv1JonHs",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-56feb4b2ab62c15c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_probs['Polish']['a'] - 0.08504245058355897) < 0.00001\n",
    "assert abs(lang_to_probs['English']['x'] - 0.001387857977519179) < 1e-5\n",
    "assert len(set(lang_to_probs['Portuguese'])) == 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IT9Az6EonHs"
   },
   "source": [
    "### Likelihoods\n",
    "Next comes the implementation of the actual classifier. First, the multinomial likelihood function `multinomial_likelihood(probs, freqs)` is implemented. Function takes two arguments: `probs` are dictionary of probabilities of each character (in some language) and `freqs` is dictionary of absolute frequencies of each character (in some text we want to classify) and return probability to obtain these absolute frequencies from multinomial distribution with given probabilities $P((X_1 = f_1) \\cap (X_2 = f_2) \\cap \\ldots \\cap (X_k = f_k))$ provided that $(X_1, \\ldots, X_k)$ is a system of multinomially distributed values with probabilities $(p_1, \\ldots, p_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KS4lto2KonHs",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9a0431bd007d04cf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def multinomial_likelihood(probs, freqs):\n",
    "    # Calculate the product of probabilities raised to the corresponding frequencies\n",
    "    product = 1\n",
    "    for char, freq in freqs.items():\n",
    "        product *= probs[char] ** freq\n",
    "    \n",
    "    # Calculate the multinomial coefficient\n",
    "    coeff = math.factorial(sum(freqs.values()))\n",
    "    for freq in freqs.values():\n",
    "        coeff //= math.factorial(freq)\n",
    "    \n",
    "    # Multiply the product and the coefficient\n",
    "    likelihood = coeff * product\n",
    "    \n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q3K8je2onHt"
   },
   "source": [
    "Let's find likelihood of data with frequencies `{'a': 2, 'b': 1, 'c': 2}` and probabilities `{'a': 0.2, 'b': 0.5, 'c': 0.3}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QoqTTC0ionHt",
    "outputId": "e625aebf-1dfd-4f46-d1bd-a5c21aa79306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05400000000000001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3}, freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2VYIB_LronHt",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0a8a6d99346b6bf8",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.054) < 0.000001\n",
    "assert abs(multinomial_likelihood(probs={'a': 0.2, 'b': 0.1, 'c': 0.3, 'd': 0.4},\n",
    "                    freqs={'a': 2, 'b': 1, 'c': 2}) - 0.0108) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8csAFr_YonHu"
   },
   "source": [
    "Сoefficient with factorials depends only on `freqs` (i.e. text that we analyse) and does not depend on `probs`. It means that for all possible languages this coefficient will be the same. Since a fixed text is considered and the probabilities for different languages are compared, it is clear that in most cases this coefficient is not needed. Next, the multinomial_likelihood_without_coeff function is implemented, which returns the same probability as multinomial_likelihood, but without the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CzH6sstIonHu",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-99bfc1e01b6b804e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def multinomial_likelihood_without_coeff(probs, freqs):\n",
    "    # Calculate the product of probabilities raised to the corresponding frequencies\n",
    "    product = 1\n",
    "    for char, freq in freqs.items():\n",
    "        product *= probs[char] ** freq\n",
    "    \n",
    "    return product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH5vtbPhonHu"
   },
   "source": [
    "Corresponding probability become smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AWxoGtbKonHu",
    "outputId": "9904bb33-64c1-41d5-9d67-034f4817ccab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0018000000000000004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RVn4twczonHv",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3de0433027c5af68",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 2}) == 0.00324\n",
    "assert abs(multinomial_likelihood_without_coeff(\n",
    "    probs={'a': 0.3, 'b': 0.4, 'c': 0.3},\n",
    "    freqs={'a': 2, 'b': 1, 'c': 5}) - 8.747999999999e-05) < 1e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j68-jIAonHv"
   },
   "source": [
    "Likelihoods become extremely small very quickly when increasing absolute frequencies in data. Unsurprisingly, the probability of getting the text that coincides with actual text from the random experiment is extremely small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QC4HF-UaonHv",
    "outputId": "2052b3e0-d56d-4e63-9a6a-e5b151878435"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00018000000000000004"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 2, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8nSWUd2fonHv",
    "outputId": "048ad04b-92d4-441e-9e55-e51579afe1b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.866455078125001e-10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                    freqs={'a': 3, 'b': 20, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i57kmLponHv"
   },
   "source": [
    "Due to the limited accuracy of computer arithmetic, exactly zero probability will be obtained for sufficiently large frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "cWeiFPtvonHv",
    "outputId": "09f518fc-17b4-44b1-9860-375fbe9a43ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                                     freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzglPanqonHw"
   },
   "source": [
    "Thus, usually such probabilities cannot be used directly. The usual way to work with such tiny numbers is to use logarithms instead of the probabilities themselves. Indeed, the logarithm is a monotonically increasing function. Comparing logarithms is equivalent to comparing their arguments.\n",
    "\n",
    "### Log likelihood\n",
    "Next, the function `log_likelihood_without_coeff(probs, freqs)` is implemented. It calculates logarithm of likelihood (without factorial coefficient). It is not logical to just put `multinomial_likelihood_without_coeff` in `log`: if the probability is extremely small (equal to zero from the computer's point of view), then `log` will be negative infinity, so the expression for the log-likelihood is first algebraically transformed. According to the properties of the logarithm: $\\log (ab) = \\log a + \\log b$, $\\log (a^b)=b\\log a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DjeFHu8MonHw",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c6283b47bbcf5b95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood_without_coeff(probs, freqs):\n",
    "    # Calculate the sum of logarithms of probabilities raised to the corresponding frequencies\n",
    "    log_likelihood = 0\n",
    "    for char, freq in freqs.items():\n",
    "        # Check if the probability is zero to avoid taking the logarithm of zero\n",
    "        if probs[char] == 0:\n",
    "            return float('-inf')\n",
    "        log_likelihood += freq * math.log(probs[char])\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyt3Yu9onHw"
   },
   "source": [
    "Likelihoods are probabilities, so they are less than 1 and their logarithms are negative. The larger absolute value of log-likelihood, the less is likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5bS5HxvponHw",
    "outputId": "fb5f1e01-e573-42e7-ef86-22ea0722b9fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.319968614080018"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 2, 'b': 1, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iToxFlQxonHw",
    "outputId": "97fefa05-e2f4-4e7d-a897-205fe2768eb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1231.2240885070605"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6yjncsWSonHx",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-38d4478fc7c1656b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                              freqs={'a': 2, 'b': 1, 'c': 2}) + 6.319968614080018) < 0.00001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512, 'c': 2}) + 1231.2240885070605) < 0.0001\n",
    "assert abs(log_likelihood_without_coeff(probs={'a': 0.2, 'b': 0.5, 'c': 0.3},\n",
    "                             freqs={'a': 543, 'b': 512}) + 1228.8161428984085) < 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSyyRqBnonHx"
   },
   "source": [
    "### Maximum likelihood classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl0iOhJPonHx"
   },
   "source": [
    "Next, the function `mle_best(text, lang_to_probs)` is implemented, which takes some `text` and dictionary `lang_to_probs` created earlier and returns the name of language such that likelihood of data for this language is maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dM8wxxcRonHx",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-761e62cc0f17dc2d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mle_best(text, lang_to_probs):\n",
    "    cleaned_text = clean_text(text)\n",
    "    freqs = get_freqs(cleaned_text)\n",
    "    best_lang = None\n",
    "    max_likelihood = float('-inf')\n",
    "    \n",
    "    for lang, probs in lang_to_probs.items():\n",
    "        log_likelihood = log_likelihood_without_coeff(probs, freqs)\n",
    "        if log_likelihood > max_likelihood:\n",
    "            max_likelihood = log_likelihood\n",
    "            best_lang = lang\n",
    "    \n",
    "    return best_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zcIgYjYEonHx",
    "outputId": "18de8347-52a5-42bb-98dc-a91c20298e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://en.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler, provisional designation 1929 SA, is a stony asteroid \n",
    "and eccentric Mars-crosser from the asteroid belt, approximately \n",
    "4 kilometers in diameter\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eebo3vi7onHy",
    "outputId": "4be101bc-9ac4-4bf6-c205-c356bafb2e88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Polish'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://pl.wikipedia.org/wiki/(1134)_Kepler\n",
    "text = \"\"\"\"(1134) Kepler – planetoida z grupy przecinających \n",
    "orbitę Marsa okrążająca Słońce w ciągu 4 lat i 145 dni \n",
    "w średniej odległości 2,68 au.\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "z5-vM4T5onHy",
    "outputId": "717d4028-90f9-4ce6-dedf-c6f17c9bb7df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italian'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source: https://it.wikipedia.org/wiki/1134_Kepler\n",
    "text = \"\"\"1134 Kepler è un asteroide areosecante. Scoperto nel 1929, \n",
    "presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \n",
    "UA e da un'eccentricità di 0,4651458, inclinata di 15,17381° rispetto\n",
    "\"\"\"\n",
    "mle_best(text, lang_to_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2_0M8_p8onHy",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b83cf8d4bcee2ab1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "assert mle_best(lines[0], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[1], lang_to_probs) == 'Portuguese'\n",
    "assert mle_best(lines[2], lang_to_probs) == 'French'\n",
    "assert mle_best(lines[3], lang_to_probs) == 'Italian'\n",
    "assert mle_best(lines[4], lang_to_probs) == 'Polish'\n",
    "assert mle_best(lines[5], lang_to_probs) == 'English'\n",
    "assert mle_best(\"Aaaa\", lang_to_probs) == \"Portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7dyK3uJonHy"
   },
   "source": [
    "Everything is working!\n",
    "\n",
    "Next, I tried to improve the resulting algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYWQTULZonHy"
   },
   "source": [
    "### Bayes rule\n",
    "\n",
    "At the moment, the probability of getting a random article from Wikipedia in English is higher than, for example, in Polish. Bayes' rule can be used to take this information into account. The Bayesian approach considers the prior probabilities of languages, then uses Bayes' rule to find their posterior probabilities and selects the language with the highest posterior probability.\n",
    "\n",
    "Let's start with finding prior probabilities. First, a lang_to_prior dictionary is created, the keys of which are the names of the languages, and the values are the prior probabilities. Let the priors be proportional to the number of articles in each language (in thousands). Let's use these numbers (in thousands): English: 6090, Italian: 1611, Spanish: 1602, German: 2439, French: 2222, Polish: 1412, Portuguese: 1034. All previous probabilities should add up to 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RnDs7oOeonHy",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1076e90562dad99c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "total_articles = 6090 + 1611 + 1602 + 2439 + 2222 + 1412 + 1034\n",
    "lang_to_prior = {\n",
    "    'English': 6090 / total_articles,\n",
    "    'Italian': 1611 / total_articles,\n",
    "    'Spanish': 1602 / total_articles,\n",
    "    'German': 2439 / total_articles,\n",
    "    'French': 2222 / total_articles,\n",
    "    'Polish': 1412 / total_articles,\n",
    "    'Portuguese': 1034 / total_articles,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "o4QVmRdzonHz",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19765ecb964a7f7f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(lang_to_prior['French'] - 0.13540524070688603) < 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXhgYKFJonHz"
   },
   "source": [
    "Next, the function `bayesian_best(text, lang_to_probs, lang_to_prior)`is implemented, which takes some text `text`, dictionary `lang_to_probs` created before and `lang_to_prior` with prior probabilities. This function have to return language name with largest posterior probability. Since only the posterior probabilities are being compared, the denominator in Bayes' Rule can be ignored: it is the same for all languages. In this process uses the logarithms of the posterior values, not the posterior values themselves, to avoid extremely small numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "A7AFgrY9onHz",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1725db02a8cb77ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def bayesian_best(text, lang_to_probs, lang_to_prior):\n",
    "    text = clean_text(text)\n",
    "    freqs = get_freqs(text)\n",
    "    \n",
    "    best_lang = None\n",
    "    best_log_posterior = -math.inf\n",
    "    \n",
    "    for lang, probs in lang_to_probs.items():\n",
    "        log_prior = math.log(lang_to_prior[lang])\n",
    "        log_likelihood = 0\n",
    "        \n",
    "        for char, freq in freqs.items():\n",
    "            if char in probs:\n",
    "                log_likelihood += freq * math.log(probs[char])\n",
    "            else:\n",
    "                log_likelihood = -math.inf\n",
    "                break\n",
    "        \n",
    "        log_posterior = log_prior + log_likelihood\n",
    "        \n",
    "        if log_posterior > best_log_posterior:\n",
    "            best_log_posterior = log_posterior\n",
    "            best_lang = lang\n",
    "    \n",
    "    return best_lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro9QObUJonHz"
   },
   "source": [
    "For example, MLE algorithm believes that word `\"The\"` belongs go German language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "N6_gz6rbonHz",
    "outputId": "fd04fd11-d1b3-4416-b6d3-7697c6f71408"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_best(\"The\", lang_to_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCH74x-GonHz"
   },
   "source": [
    "However, if we take into account that English is more popular in Wikipdia, results changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "v8LfC6ToonHz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'English'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_best(\"The\", lang_to_probs, lang_to_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KvlT6sBConHz",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d6b6b54026f24fb0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lines = ['Aaa', 'Aa', 'Kepler è un asteroide areosecante. Scoperto nel',\n",
    "        \"presenta un'orbita caratterizzata da un semiasse maggiore pari a 2,6829098 \"\n",
    "         \"UA e da un'eccentricità di 0,4651458, inclinata\",\n",
    "         \"Kepler – planetoida z grupy przecinających orbitę Marsa okrążająca Słońce w ciągu 4 lat i \",\n",
    "         \"Kepler, provisional designation 1929 SA, is a stony asteroid \"\n",
    "         \"and eccentric Mars-crosser from the\"]\n",
    "answers = ['English', 'English', 'French', 'Italian', 'Polish', 'English']\n",
    "for line, answer in zip(lines, answers):\n",
    "    assert bayesian_best(line, lang_to_probs, lang_to_prior) == answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9RMCkxfonH0"
   },
   "source": [
    "### Measuring uncertainty\n",
    "\n",
    "The next step is to get the posterior probability of how accurate the created Bayesian algorithm is in its classification. The function `bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang)` is implemented below, that takes some `text`, `lang_to_probs`, `lang_to_prior` and a particular language `test_lang` of interest and returns posterior probability for this language provided this text.\n",
    "\n",
    "In this case, the actual probabilities (likelihoods) are used, not their logarithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "c0epuiy4onH0",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8705f891f8eff4f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_posterior(text, lang_to_probs, lang_to_prior, test_lang):\n",
    "    # Calculate the likelihood of the text for the given language\n",
    "    likelihood = 1.0\n",
    "    for word in clean_text(text):\n",
    "        if word in lang_to_probs[test_lang]:\n",
    "            likelihood *= lang_to_probs[test_lang][word]\n",
    "        else:\n",
    "            likelihood *= lang_to_probs[test_lang]['<UNK>']\n",
    "    \n",
    "    # Calculate the denominator of the Bayes' rule formula\n",
    "    denominator = 0.0\n",
    "    for lang in lang_to_probs.keys():\n",
    "        lang_likelihood = 1.0\n",
    "        for word in clean_text(text):\n",
    "            if word in lang_to_probs[lang]:\n",
    "                lang_likelihood *= lang_to_probs[lang][word]\n",
    "            else:\n",
    "                lang_likelihood *= lang_to_probs[lang]['<UNK>']\n",
    "        denominator += lang_likelihood * lang_to_prior[lang]\n",
    "    \n",
    "    # Calculate the posterior probability for the given language\n",
    "    posterior = likelihood * lang_to_prior[test_lang] / denominator\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mAOwAYnponH0",
    "outputId": "8502a5c0-1610-467b-9545-d91ce1c9779c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2686381464045339"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "sUVZPTiPonH0",
    "outputId": "703c7d81-817c-479d-f8ec-358688a8deeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.534626604689872"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NdIeqtygonH0",
    "outputId": "b73f8a5e-1a84-43eb-fffc-b19bb9146fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36596793151737517"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "pm21A6zvonH0",
    "outputId": "07633b01-904b-4598-a7a4-6c3a74f73402"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12095519926831602"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fsuKSxNmonH0",
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-23ecc58928d8d618",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"German\") - 0.26863814640) < 0.00001\n",
    "assert abs(bayesian_posterior(\"The\", lang_to_probs, lang_to_prior, \"English\") - 0.534626604) < 0.00001\n",
    "assert abs(bayesian_posterior(\"Das\", lang_to_probs, lang_to_prior, \"English\") - 0.365967931517) < 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NY-Bm5lonH0"
   },
   "source": [
    "Algorithm believes that `\"Das\"` belongs to English. This is due to small amount of data (only three letters!) and high prior for English. However, it is not very certain: the posterior is only 0.37. On the other hand, `\"The\"` belongs to `\"English\"` with much larger posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kiewa34NonH0"
   },
   "source": [
    "## Conclusions\n",
    "This project uses my knowledge of probability to build classifier algorithm. In fact, this is a version of the famous Naive Bayes classifier. Of course, this classifier is far from perfect: for example, it completely ignores words and only deals with characters and their frequencies. However, it works pretty well despite being so simple. It also shows several important concepts: probability, maximum likelihood estimate, Bayesian estimates, and so on.\n",
    "\n",
    "This project uses dictionaries to represent frequency tables and pure Python constructs such as loops to process them. To make this algorithm more efficient, numpy arrays and vectorized functions can be used."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
